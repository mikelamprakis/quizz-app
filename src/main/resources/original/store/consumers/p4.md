## Question 22

What happens when a consumer with `isolation.level=read_committed` encounters a message that is part of an ongoing transaction?

* A. The consumer will read the message immediately
* B. The consumer will wait until the transaction is committed before reading the message
* C. The consumer will skip the message and move on to the next one
* D. The consumer will throw an exception and stop consuming

<details><summary>Response:</summary>

**Answer:** B

**Explanation:**
When using `read_committed`, the consumer will wait until the transaction is completed (committed) before making any message from that transaction visible. This ensures transactional consistency and avoids consuming messages that might later be rolled back.

</details>


Here are the adjusted questions in the format you provided:

---

## Question 23

What is the purpose of the `max.poll.records` setting in the Kafka consumer configuration?

* A. To specify the maximum number of records to return in a single poll
* B. To control the maximum amount of data the consumer can receive per second
* C. To set the maximum number of partitions the consumer can subscribe to
* D. To determine the maximum number of consumers allowed in a consumer group

<details><summary>Response:</summary>

**Answer:** A

**Explanation:**
The `max.poll.records` setting specifies the maximum number of records returned in a single call to `poll()`. This allows the consumer to limit the number of records fetched and processed in one batch, helping to control memory usage and processing latency. The default is 500.

</details>

---

## Question 24

How does the `max.poll.interval.ms` setting affect the behavior of a Kafka consumer?

* A. It specifies the maximum amount of time the consumer can wait before polling for new records
* B. It sets the maximum interval between two consecutive polls before the consumer is considered dead
* C. It determines the maximum time allowed for message processing before committing offsets
* D. It controls the maximum number of records the consumer can poll in a single request

<details><summary>Response:</summary>

**Answer:** B

**Explanation:**
`max.poll.interval.ms` sets the maximum time between two calls to `poll()`. If this interval is exceeded, the consumer is considered unresponsive and removed from the group. This helps Kafka detect and recover from stuck or slow consumers.

</details>

---

## Question 25

What happens when a Kafka consumer is marked as dead due to exceeding the `max.poll.interval.ms` interval?

* A. The consumer is automatically rebalanced, and its partitions are reassigned to other consumers in the group
* B. The consumer receives an exception and must manually rejoin the consumer group
* C. The consumer's offset commits are rolled back, and it starts consuming from the beginning of the assigned partitions
* D. The consumer is permanently removed from the consumer group and cannot rejoin

<details><summary>Response:</summary>

**Answer:** A

**Explanation:**
If a consumer exceeds the `max.poll.interval.ms` threshold, Kafka triggers a rebalance. The partitions it was responsible for are reassigned to other consumers. The dead consumer can later rejoin the group and receive new assignments.

</details>

---

## Question 26

What triggers a partition rebalance in a Kafka consumer group?

* A. Adding a new topic to the Kafka cluster
* B. Changing the replication factor of a topic
* C. Adding a new consumer to the consumer group
* D. Modifying the consumer group ID

<details><summary>Response:</summary>

**Answer:** C

**Explanation:**
A rebalance is triggered when the composition of the consumer group changes â€” for example, when a new consumer joins or an existing one leaves. Kafka redistributes partitions among the current consumers to maintain balance.

</details>

---

## Question 27

What happens to the partition assignments during a consumer group rebalance?

* A. Partitions are evenly distributed among the remaining consumers
* B. Partitions are assigned to the consumers based on the consumer group ID
* C. Partitions are randomly assigned to the consumers
* D. Partitions are assigned to the consumers based on the topic name

<details><summary>Response:</summary>

**Answer:** A

**Explanation:**
During a rebalance, Kafka redistributes partitions among all active consumers. Depending on the partition assignment strategy (e.g., range or round-robin), it aims to balance the load across consumers as evenly as possible.

</details>

---

## Question 31

How can you minimize the impact of consumer group rebalances in a Kafka application?

* A. Increase the session timeout value for consumers
* B. Reduce the number of partitions for the consumed topics
* C. Implement a custom partition assignment strategy
* D. Use static group membership for consumers

<details><summary>Response:</summary>

**Answer:** D

**Explanation:**
Using static group membership (by setting `group.instance.id`) helps consumers retain their partition assignments during restarts, which avoids triggering a full group rebalance and reduces downtime or overhead.

</details>

---

